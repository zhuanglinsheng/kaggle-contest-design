%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Author template for Management Science (mnsc) for articles with no e-companion (EC)
%% Mirko Janc, Ph.D., INFORMS, mirko.janc@informs.org
%% ver. 0.95, December 2010
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[mnsc]{informs3}
%\documentclass[mnsc,blindrev]{informs3}
%\documentclass[mnsc,nonblindrev]{informs3} % current default for manuscript submission

\OneAndAHalfSpacedXI
%%\OneAndAHalfSpacedXII % Current default line spacing
%%\DoubleSpacedXII
%%\DoubleSpacedXI

% If hyperref is used, dvi-to-ps driver of choice must be declared as
%   an additional option to the \documentclass. For example
%\documentclass[dvips,mnsc]{informs3}      % if dvips is used
%\documentclass[dvipsone,mnsc]{informs3}   % if dvipsone is used, etc.

% Private macros here (check that there is no clash with the style)
%%%%%%%% Linkage
\usepackage{xurl}
\usepackage{hyperref}
\hypersetup{colorlinks=true,citecolor=blue,urlcolor=blue}

%%%%%%%% Colored underline
\usepackage{ulem}
\usepackage{soul}
\makeatletter
    \newcommand{\coloruline}[2]{%
        \newcommand\temp@reduline{\bgroup\markoverwith
            {\textcolor{#1}{\rule[-0.5ex]{2pt}{0.4pt}}}\ULon}%
        \temp@reduline{#2}%
    }
    \newcommand{\coloruwave}[2]{%
        \UL@protected\def\temp@uwave{\leavevmode \bgroup 
        \ifdim \ULdepth=\maxdimen \ULdepth 3.5\p@
        \else \advance\ULdepth2\p@ 
        \fi \markoverwith{\textcolor{#1}{\lower\ULdepth\hbox{\sixly \char58}}}\ULon}
        \font\sixly=lasy6 % does not re-load if already loaded, so no memory drain.
        \temp@uwave{#2}%
    }
\makeatother

%%%%%%%% Algorithm
\usepackage{algorithm}
\usepackage{algorithmic}

%%%%%%%% Table, Figure and Diagram
%%%%%%%%%% Table
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
%%%%%%%%%% Figure
\usepackage{float}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{caption,}
\usepackage{subcaption}
%%%%%%%%%% Diagram Figure
\usepackage{tikz}
\usepackage{varwidth}
%%%%%%%%%% Comments
\usepackage{comment}


% Natbib setup for author-year style
\usepackage{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%

%% Setup of theorem styles. Outcomment only one.
%% Preferred default is the first option.
\TheoremsNumberedThrough     % Preferred (Theorem 1, Lemma 1, Theorem 2)
%\TheoremsNumberedByChapter  % (Theorem 1.1, Lema 1.1, Theorem 1.2)
\ECRepeatTheorems

%% Setup of the equation numbering system. Outcomment only one.
%% Preferred default is the first option.
\EquationsNumberedThrough    % Default: (1), (2), ...
%\EquationsNumberedBySection % (1.1), (1.2), ...

% For new submissions, leave this number blank.
% For revisions, input the manuscript number assigned by the on-line
% system along with a suffix ".Rx" where x is the revision number.
\MANUSCRIPTNO{MS-0001-1922.65}


%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%

% Outcomment only when entries are known. Otherwise leave as is and
%   default values will be used.
%\setcounter{page}{1}
%\VOLUME{00}%
%\NO{0}%
%\MONTH{Xxxxx}% (month or a similar seasonal id)
%\YEAR{0000}% e.g., 2005
%\FIRSTPAGE{000}%
%\LASTPAGE{000}%
%\SHORTYEAR{00}% shortened year (two-digit)
%\ISSUE{0000} %
%\LONGFIRSTPAGE{0001} %
%\DOI{10.1287/xxxx.0000.0000}%

% Author's names for the running heads
% Sample depending on the number of authors;
% \RUNAUTHOR{Jones}
% \RUNAUTHOR{Jones and Wilson}
% \RUNAUTHOR{Jones, Miller, and Wilson}
% \RUNAUTHOR{Jones et al.} % for four or more authors
% Enter authors following the given pattern:
%\RUNAUTHOR{}

% Title or shortened title suitable for running heads. Sample:
% \RUNTITLE{Bundling Information Goods of Decreasing Value}
% Enter the (shortened) title:
\RUNTITLE{Information Disclosure in Dynamic Innovation Contests}

% Full title. Sample:
% \TITLE{Bundling Information Goods of Decreasing Value}
% Enter the full title:
\TITLE{Information Disclosure in Dynamic Innovation Contests}

% Block of authors and their affiliations starts here:
% NOTE: Authors with same affiliation, if the order of authors allows,
%   should be entered in ONE field, separated by a comma.
%   \EMAIL field can be repeated if more than one author
\ARTICLEAUTHORS{%
\AUTHOR{Jussi Keppo}
\AFF{NUS Business School and Institute of Operations Research and Analytics\\
	National University of Singapore, Singapore\\
	\EMAIL{keppo@nus.edu.sg}} %, \URL{}}
\AUTHOR{Linsheng Zhuang}
\AFF{Institute of Operations Research and Analytics\\
	National University of Singapore, Singapore\\
	\EMAIL{linsheng.z@u.nus.edu}}
% Enter all authors
} % end of the block

\ABSTRACT{%
...
% Enter your abstract
}%

% Sample
%\KEYWORDS{deterministic inventory theory; infinite linear programming duality;
%  existence of optimal policies; semi-Markov decision process; cyclic schedule}

% Fill in data. If unknown, outcomment the field
\KEYWORDS{Digital Economy, Data Protection Regulation, Innovation Contest} 
%\HISTORY{......}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Samples of sectioning (and labeling) in MNSC
% NOTE: (1) \section and \subsection do NOT end with a period
%       (2) \subsubsection and lower need end punctuation
%       (3) capitalization is as shown (title style).
%
%\section{Introduction.}\label{intro} %%1.
%\subsection{Duality and the Classical EOQ Problem.}\label{class-EOQ} %% 1.1.
%\subsection{Outline.}\label{outline1} %% 1.2.
%\subsubsection{Cyclic Schedules for the General Deterministic SMDP.}
%  \label{cyclic-schedules} %% 1.2.1
%\section{Problem Description.}\label{problemdescription} %% 2.

% Text of your paper here


\section{Introduction}

\begin{itemize}
\item Kaggle\footnote{\url{https://www.kaggle.com}} ...
\item Meta-kaggle dataset \cite{megan_risdal_timo_bozsolik_2022}. 
\end{itemize}

\subsection{Literature Review}

This paper focuses on the two players innovation contest with a continuous time  where the players’ relative position is public information throughout the game.
This is closely related to tug-or-war contest, which, to our knowledge, was first formally given by \citet{Harris1987Race} as a one-dimensional simplification of the multi-stage R\&D race. 
The output processes are model by Brownian motions drifted with effort inputs, which is followed by \cite{budd1993model} who model the state of a dynamic competition of two innovative duopoly firms by a Brownian motion drifted by the effort gap, and solve the equilibrium approximately. 
Furthermore, \citet{Moscarini2007Contest} model the tug-of-war state as the gap of the two outputs directly, and draw an analytical equilibrium of the pure strategies. 

...

Information disclosure in contest - \cite{Bimpikis2019Contest}. 

...

Closest paper - \cite{ryvkin2022fight}.

...




\section{The Model}

...

\subsection{Settings}

% Effort, Output and Gap
We assume two players, $i$ and $j$, compete for a prize $\theta>0$ in a contest. 
Winner gets the prize and loser gets nothing. 
The contest starts at time zero. 
At every time $t\ge0$, the representative player $i$ chooses an effort level $q_{i,t}$ and burdens a quadratic cost $C_i(q_{i,t}) = c_i q_{i,t}^2/2$, with a lower $c_i$ corresponding to higher ability. 
The dynamic of player $i$'s output follows 
\begin{equation}
dx_{i,t} = q_{i,t}dt + \sigma_idW_{i,t}
\end{equation}
Here $W_{i,t}$ is a standard Brownian motion and $\sigma_i>0$ measures her production risk. 
Moreover, denoted by $y_t$ the \textit{output gap} of player $i$ and $j$ at time $t$. 



\subsubsection{Kalman Filter.}

We assume that the contest is equipped with a submission system that allows participants to upload their algorithms at any time and receive immediate feedback. 
For simplicity, we further assume that agents submit their intermediate results whenever they make progress. 
This setup enables the contest organizers to monitor all players’ progress $x_{i,t}$ and $x_{j,t}$ in real time (whenever there is a submission). 
The true outputs, evaluated by the system, is only known by the game designer but not the two players. 

% Submission
Let' suppose the submission events of the representative player $i$ occur at times $(t^i_1, t^i_2, ...)$ following an inhomogeneous Poisson process driven by the intensity function $\tau_i(t)$. 

% Signal
After each submission, the contest designer emits a \textit{public} signal of the real output $x_{i,t_k}$. 
The signal is ambiguous and the game holder controls the ambiguity. 
The dynamic of signal is 
\begin{equation}\label{signal}
	\hat{x}_{i,k} = x_{i,t_k} + \frac{v_{i,k}}{\sqrt{\lambda}}, \quad k=1, 2, ...
\end{equation}
where $v_{i,k}$ follows standard normal distribution and is independent with $(W_{i,t})$ and $(W_{j,t})$, and the parameter $\lambda$ is set by the game holder to control the precision of signals. 
The larger the $\lambda$, the more accurate the signal would be. 

% Bayesian Player
The information set of both players at time $t \ge 0$ is  $I_{t} := \{\hat{x}_{i,k}, \hat{x}_{j,k} : 0\le t_k \le t\}$. 
Both players estimate the unknown outputs $x_{i,t}$ and $x_{j,t}$ purely based on the information set $I_t$ and hidden actions $q_{i,t}$ and $q_{j,t}$. 
Let $\tilde{x}_{i,t} \equiv E(x_{i,t}|I_t)$ be the estimated output gap and $S_{i,t} \equiv E[(\tilde{x}_{i,t}-x_{i,t})^2|I_t]$ be the estimation variance. 
The conditional distribution $y_{t}|I_t\sim\mathcal{N}(\tilde{y}_{t},S_{t}|I_t)$ is fully captured by the mean $\tilde{y}_{t}$ and variance $S_{t}$. 

The evolution of $\tilde{x}_{i,t}$ and $S_{i,t}$ is characterized by a continuous-discrete Kalman Filter (CD-KF, \citealt{barrau2017invariant}, \citealt{frogerais2012various}), with the measurement of each step $k=1, 2, ...$ consisting of two phases: in
(1) prediction phase during time interval $(t_{k-1}, t_{k})$, equations are derived from those of Kalman-Bucy filter \citep{Bensoussan1992Control} without considering the Kalman gain: 
\begin{align}
	d\tilde{x}_{i,t} &= q_{i,t}dt
	\label{filtered-x}\\
	dS_{i,t} &= \sigma^2dt
	\label{filtered-S}
\end{align} 
and in (2) updation phase $t=t_k$, equations are 
\begin{align}
	\tilde{x}_{i,k}^{+} &= \tilde{x}_{i,k}^{-} + \frac{\lambda S_{i,t_k}^{-}}{\lambda S_{i,t_k}^{-}+1}\left(\hat{x}_{i,k} - \tilde{x}_{i,t_k}^{-}\right)
	\label{updated-x}\\
	S_{i,t_k}^{+} &= \frac{S_{i,t_k}^{-}}{\lambda S_{i,t_k}^{-}+1} 
\end{align}
If $\lambda = 0$, we have $S_t = S_0+\sigma^2t$, i.e., the estimation variance is increasing in time linearly. 
If $\lambda > 0$, the estimation error decreases after each update of the public leaderboard, i.e., $S_{i,t_k}^+ < S_{i,t_k}^-$. 





\subsubsection{Dynamic Contest.}

% Contest Deadline
Following \cite{ryvkin2022fight}, let's consider a dynamic contest of two players with a fixed deadline. 
Suppose the contest is terminated when time $t=T>0$. 
Since the steady state estimation variance $\bar{S}$ is fixed as displayed above, the state of the game is fully characterized by a tuple $(\tilde{y}_t, t)$. 
At any time $0\le t<T$, player $i$ optimizes her effort level $q_{i,\tau}$ in the remaining contest period $\tau\in[t, T)$ according to the following optimization problem, 
\begin{equation}\label{v-def}
	V^i(\tilde{y}_{t}, t ; q_{j,t},\Theta_i) = \max_{\{q_{i,\tau}\}^T_{\tau=t}} 
	\mathbb{E}\left( \theta\cdot1_{\tilde{y}_T>0} - \int^T_tC_i(q_{i,\tau})d\tau \bigg|I_t\right) 
\end{equation}
where $\Theta_i \equiv\{\theta, \lambda, \sigma, c_i\}$, subject to constraints (\ref{filtered-x}), (\ref{filtered-S}) and $q_{i,\tau}\ge0$ for all $\tau\in[t,T)$.
The optimization problem for player $j$ is just symmetric to that of player $i$ as $V^j(\tilde{y}_t, t) = V^i(-\tilde{y}_t, t)$. 
The corresponding Hamilton-Jacobi-Bellman (HJB) equation for player $i$ is 
\begin{equation*}
0 = \max_{q_{i,t}\ge0}\left[-\frac{c_iq_i^2}{2} + V^i_{y}\cdot\left(q_{i,t}-q_{j,t}\right)+V^i_t + \frac{V^i_{yy}}{2}\lambda \bar{S}^2\right]
\end{equation*}
By definition, we have $\lambda \bar{S}^2 = \sigma^2$. 
Under the assumption of inner solution, we plug into the first order conditions $q_{i,t} = V^i_y/c_i$ and $q_{j,t} = -V^j_y/c_j$, we have the system of equations
\begin{equation*}
\begin{aligned}
\frac{1}{2c_i}(V^i_y)^2 + \frac{1}{c_j}V^i_yV^j_y + V^i_t + V^i_{yy}\frac{\sigma^2}{2} = 0\\
\frac{1}{2c_j}(V^j_y)^2 + \frac{1}{c_i}V^j_yV^i_y + V^j_t + V^j_{yy}\frac{\sigma^2}{2} = 0
\end{aligned}
\end{equation*}
subject to boundary conditions $V^i(-\infty, t) = 0$, $V^i(+\infty, t) = \theta$, $V^j(-\infty, t) = \theta$, $V^j(+\infty, t)=0$, $V^i(\tilde{y}_T, T) = \theta \cdot 1_{\tilde{y}_T > 0}$ and $V^j(\tilde{y}_T, T) = \theta \cdot 1_{\tilde{y}_T < 0}$. 


The Nash equilibrium is summarized in the following lemma. 
We include a simplified version of the proof in the appendix: 

\begin{lemma}[\citealt{ryvkin2022fight}]
In the Markov perfect equilibrium, the players’ efforts in state $(y, t) \in\mathbb{R}\times[0, T)$ are given by
\begin{equation}\label{eq-equilibrium-effort}
m_{i{j}}(y, t) = \frac{e^{-z^2/2}}{\sqrt{2\pi\sigma^2(T-t)}}\cdot \frac{\sigma^2}{2}\left[\gamma(\rho_{i}) + \gamma(\rho_{j})\right]\left[1-\rho(z)^2\right]\left[1 \pm \rho(z)\right]
\end{equation}
where $z = y / (\sigma\sqrt{T-t})$, $\rho(z) = \gamma^{-1}\left(\Phi(z)\left[\gamma(\rho_{i})+\gamma(\rho_{j})\right]-\gamma(\rho_{j})\right)$ and 
\begin{equation*}
\gamma(u) = \frac{u}{1-u^2} + \frac{1}{2}\ln\frac{1+u}{1-u},\quad u\in(-1,1)
\end{equation*}
\begin{equation*}
\rho_{i} = \frac{e^{w_{i}}+e^{-w_{j}}-2}{e^{w_{i}}-e^{-w_{j}}},
\quad
\rho_{j} = \frac{e^{w_{j}}+e^{-w_{i}}-2}{e^{w_{j}}-e^{-w_{i}}},
\quad
w_{i(j)} = \frac{\theta}{\sigma^2 c_{i(j)}}.
\end{equation*}
\end{lemma}

The variables $w_{i(j)}$ represent the abilities of two players, while $\rho_{i(j)}$ normalizes $w_{i(j)}$ into the interval $(-1, 1)$. 
It is not hard to see that $\gamma(\cdot)$ is strictly increasing on $(-1,1)$, ranging from $-\infty$ to $+\infty$. 
Moreover, the equilibrium effort $m_{i(j)}$ can be represented to the product of $\phi(y; 0, \sigma^2(T-t))$, the probability density of normal distribution with mean zero and variance $\sigma^2(T-t)$ at the state $y$ and an amplitude factor that only depends on the composite variable $z$. 

Figure...







\section{Model Estimation}

In this section, we describe the estimation procedure. 
We first outline the data generation process, establishing the connection between the empirical data and the theoretical model discussed previously.
Then, we introduce a structural estimation method using Bayesian framework.

\subsection{Data Generating Process}

For each contest on Kaggle, the observable information can be classified into three primary components: 

%% Data: Contest setting
The first component consists of essential contest details, including the contest duration, prize structure, information disclosure, and other governing rules.
Contrary to the assumptions of our model, a typical contest usually involves multiple teams rather than just two.
We index the participating teams of the contest by $i\in\{1, 2, ..., n\}$. 
To fully leverage the potential of the data and establish a connection with our theoretical model, let's assume that each participant perceives a competitor they are playing against at every moment, denoted as $j$. 
This perceived competitor is typically understood as the most prominent individual on the leaderboard, i.e., the person ranked first.
When team $i$ themselves hold the top position, their perceived competitor is the individual who poses the greatest threat, namely the person ranked second.
Furthermore, the contests may feature intricate prize structures, such as the provision of multiple awards, rather than adhering to a winner-takes-all format.
The issue will be discussed in Section~\ref{sec-kaggle-application}. 

%% Data: Submissions
The second component captures the submission events of each player $i$ to the system, denoted by the sequence $\{\hat{t}^i_k\}_{k=1}^{N_i}$.
Here, $N_i$ represents for the total number of submissions by player $i$, and $t$ represents for the time of each submission. 
We understand the submission events of player $i$ and $j$ as two conditional independent inhomogeneous Poisson processes, driven by the intensity functions $\tau_i(t)$ and $\tau_j(t)$.\footnote{That is, given the intensity functions $\tau_i(t)$ and $\tau_j(t)$, the submission events $\{\hat{t}^i_k\}_{k=1}^{N_i}$ and $\{\hat{t}^j_k\}_{k=1}^{N_i}$ are mutually independent.}
Then, during any time interval $\mathcal{S}$ of the contest duration $\mathcal{T}$, the Poisson arrival rate of submissions of the representative player $i$ is given by $\int_{s\in\mathcal{S}}\tau_i(s)ds$. 
We assume the submission intensity $\tau_i(t)$ is proportional to the effort level $m_i(\tilde{y}_t, t)$. More specifically, 
\begin{equation}\label{eq-model-intensity}
\tau_i(t) = r \cdot m_i(\tilde{y}_t, t)
\end{equation}
where $r>0$ is the common ratio of submission intensity and effort. 

%% Data: Leaderboard
The third component of the observed data is an open leaderboard that records the real-time rankings and scores of each participant, denoted by $\hat{x}^i_t$. 
We interpret the difference in scores between $i$ and $j$ displayed on the leaderboard as the signal $Z_t$ (defined in (\ref{signal})) released by the contest organizer. 
Specifically, let's denote $\hat{y}_t = \hat{x}^i_t - \hat{x}^j_t$ the gap between displayed scores and
\begin{equation}\label{eq-model-signal}
dZ_t = \hat{y}_tdt
\end{equation}
As indicated in (\ref{signal}), we assume that $\hat{y}_t$ represents a noisy signal, with its precision controlled by $\lambda$. 

In practice, the leaderboard signals are inherently noisy, as organizers deliberately disclose only a subset of the full dataset to participants to mitigate the risk of overfitting. 
The proportion of the released data is generally known to all participants.
In addition to the public leaderboard, most competitions hosted on Kaggle also maintain a private leaderboard, where organizers evaluate the true predictive performance of participants’ models using the full dataset.

Beyond the uncertainty introduced by partial data disclosure, a second source of signal noise arises from the timing of leaderboard updates: 
rankings are refreshed only after model submissions, creating a delay relative to the true standings.
It should be noted that such lag would bias our model estimates only if participants strategically timed their submissions.
Although such strategic behaviour is theoretically possible, we abstract from it in this study.
We assume that each submission follows a period of substantive effort, allowing the leaderboard rankings to broadly reflect the relative performance of algorithms based on the publicly available data.




\subsection{Bayesian Framework}

The likelihood function of any realization of this point process $\{\hat{t}^i_k\}_{k=1}^{N_i}$ is given by 
\begin{equation}\label{eq-ihpp-prob}
p\left(\{\hat{t}^i_k\}_{k=1}^{N_i} | \tau_i\right) = \exp\left\{-\int_{s\in\mathcal{T}}\tau_i(s)ds\right\}\prod_{k=1}^{N_i}\tau_i(\hat{t}^i_k)
\end{equation}



By (\ref{filtered-x}) and (\ref{eq-model-signal}), the estimated gap $\tilde{y}_t$ by the two players follows 
\begin{equation}\label{eq-fintered-y-update}
d\tilde{y}_{t} = (q_{i,t}-q_{j,t})dt + \sqrt{\lambda}\sigma(\hat{y}_t-\tilde{y}_{t}) dt
\end{equation}

Unknown parameters to be estimated: 
\begin{itemize}
	\item $\sigma$ (Total innovation risk): prior 
	\item $c_i$ and $c_j$ (Capacities)
\end{itemize}

\subsection{Estimation Procedure}

...

\subsubsection{Discrete time.} 

...


\section{Application}

...

\subsection{Synthetic Data}

Before applying our estimation procedure to real-world contest data, we evaluate its potential on synthetically generated data.

Based on the following assumptions, we will next generate the submission data for two players in a data analysis competition: 
\begin{itemize}
	\item Total innovation uncertainty of the two players per day: $\sigma = 5$
	\item Capacities: $c_i = 1.5$ and $c_j = 2$
	\item Contest duration: from 2025-01-01 to 2025-03-31
	\item Starting point $\tilde{y}_0=0$
\end{itemize}

A central challenge in generating synthetic data lies in dynamically constructing a point process that conforms to an inhomogeneous Poisson process.
Inspired by the thinning technique \citep{lewis1979simulation}, we first generate candidate events according to a homogeneous Poisson process within each discrete time interval, using a fixed intensity $\tau_i^\star \ge \sup_t \tau_i(t)$, and distribute them uniformly across the interval.
We then apply the thinning procedure to determine whether each candidate event is accepted.

\begin{algorithm}
\caption{Synthetic Data Simulation}
\begin{algorithmic}
\STATE \textbf{Input:} 
	$c_i$, $c_j$, $\sigma$, $\lambda$, $r$, $\tau^\star$, $y_0$, $\tilde{y}_0$
\STATE Sample $\{s_k^{i(j)}\}^{N_{i(j)}}_{k=1}$ from homogeneous Poisson process $(\tau^\star_{i(j)})$ on $[0, T)$
\STATE Sample $\{u_k^{i(j)}\}^{N_{i(j)}}_{k=1}$ from uniform distribution on $[0, 1]$
\STATE Initialize $\ell=0$, $\ell^{i(j)}=0$, $\hat{y}_0 = 0$
\FOR{$t = 0$ to $T$}
    \STATE $m_{i(j)}(\tilde{y}_t, t)$  $\gets$ (\ref{eq-equilibrium-effort}), $\tau^{i(j)}(t)$ $\gets$ (\ref{eq-model-intensity})
    \STATE $\tilde{y}_{t+\Delta t}$ $\gets$ (\ref{eq-fintered-y-update})
    \FOR{ each $s_{k}^{i(j)} \subset [t, t+\Delta t)$} 
        \IF {$u_k^{i(j)} > \tau_{i(j)}(s_{k}^{i(j)}) / \tau^\star_{i(j)}$}
        	    \STATE $\hat{t}^{i(j)}_{\ell} \gets s_{k}^{i(j)}$; $\ell^{i(j)} \gets \ell^{i(j)}+1$
	    \hfill \COMMENT{Accept the submission event}
	    \STATE $\hat{y}_{\ell} \gets $; $\ell \gets \ell + 1$
	    \hfill \COMMENT{Update public leaderboard}
        \ENDIF
    \ENDFOR
\ENDFOR
\STATE \textbf{Output:} $(y_t)$, $(\tilde{y}_t)$, $\{\hat{t}_k^{i(j)}\}^{N_{i(j)}}_{k=1}$, $\{\hat{y}_k\}_{k=1}^{N_i + N_j}$
\end{algorithmic}
\end{algorithm}



\subsection{Case Study}\label{sec-kaggle-application}

...


\newpage
\section{Conclusion}


% Appendix here
% Options are (1) APPENDIX (with or without general title) or
%             (2) APPENDICES (if it has more than one unrelated sections)
% Outcomment the appropriate case if necessary
%
% \begin{APPENDIX}{<Title of the Appendix>}
% \end{APPENDIX}
%
%   or
%
% \begin{APPENDICES}
% \section{<Title of Section A>}
% \section{<Title of Section B>}
% etc
% \end{APPENDICES}

\newpage
\begin{APPENDICES}











\end{APPENDICES}



% Acknowledgments here
%\ACKNOWLEDGMENT{......}


% References here (outcomment the appropriate case)

% CASE 1: BiBTeX used to constantly update the references
%   (while the paper is being written).
%\bibliographystyle{informs2014} % outcomment this and next line in Case 1
%\bibliography{<your bib file(s)>} % if more than one, comma separated

% CASE 2: BiBTeX used to generate mypaper.bbl (to be further fine tuned)
%\input{mypaper.bbl} % outcomment this line in Case 2

%If you don't use BiBTex, you can manually itemize references as shown below.


%\bibliographystyle{nonumber}
\bibliographystyle{informs2014}
\bibliography{_Literatures.bib}

%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%
